---
title: |
  | \vspace{5cm} Geo-Spatial Analysis of County-Level Unemployment Rates
  | and Socio-Economic Factors in California
subtitle: |
  | \vspace{1cm} STATS C173 Final Project
author: "Robin Lee"
date: "`r Sys.Date()`" 
output: 
  pdf_document:
    fig_width: 12
    toc: no
    number_sections: true
---
\newpage
```{=latex}
\setcounter{tocdepth}{3}
\tableofcontents
```
\newpage




```{r setup, include=FALSE, warning= FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', message=FALSE,cache=FALSE)
library(dplyr)
library(tidyr)
library(tidyverse)
library(geoR)
library(gstat)
library(sp)
library(maps)
library(corrplot)
library(ggplot2)
library(gridExtra)
```

```{r,echo=FALSE, include=FALSE,eval=FALSE}
data <- read.csv("epa_data.csv")
data <- data %>% 
  group_by(TRI.Facility.ID) %>%
  pivot_wider(names_from = Chemical, values_from = Releases..lb.) %>%
  rename("lead" = "Lead compounds (N420)") %>%
  mutate(Waste.Managed..lb. = as.numeric(Waste.Managed..lb.),
         lead = as.numeric(lead)) %>%
  summarize(longitude = mean(Longitude),
            latitude = mean(Latitude),
            mean_color = mean(People.of.Color.Percentile),
            mean_lowincome = mean(Low.Income.Percentile),
            mean_unemployment = mean(Unemployment.Rate.Percentile),
            mean_waste = mean(Waste.Managed..lb., na.rm=T),
            mean_lead = mean(lead, na.rm=T)) %>%
  filter(!is.na(mean_lead), !is.na(mean_waste))

map("county", "ca", main = "California Map of County Seats")
points(data$longitude, data$latitude)
```



# Introduction

## Description of the Data

This project utilizes data from the U.S. Department of Agriculture's Economic Research Service (ERS). The ERS compiles the latest statistics on socioeconomic indicators—like poverty rates, population change, unemployment rates, and education levels—and provides maps and data for U.S. States and counties/county equivalents. The data can be accessed at: https://www.ers.usda.gov/data-products/county-level-data-sets/. 

## Variable Overview

In this project, I have selected county-level unemployment rate as our target variable on which we want to make predictions. As such, the chosen predictors will consist of the following county-level attributes: median household income as a percentage of California's median household income, the percent of total population living below the poverty line, the percent of adults with a Bachelor's degree, the percent of adults with a high school diploma, the rate of change in net migration, and death rate. To ensure consistency, all measurements will be from 2021. Our final dataset consists of the following variables:

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|p{8cm}|c|}
    \hline
    Label & Description & Unit of Measure \\
    \hline
    Longitude & County Seat Longitude Position & Degrees \\
    Latitude & County Seat Latitude Position & Degrees \\
    PercentStateMedianIncome & The median household income as a percentage of the total median household income for CA & Percentage \\
    Unemployment$\_$rate$\_$2021 & Percentage of unemployed individuals in the civilian labor force for the year 2021 & Percentage \\
    PCTPOVALL$\_$2021 & The percentage of the total population living below the poverty line in the year 2021 & Percentage \\
    Bachelors$\_$2021 & The county-level percentage of adults with a bachelor's degree or higher for the year 2021 & Percentage \\
    Highschool$\_$2021 & The county-level percentage  of adults with a high school diploma only for the year 2021 & Percentage \\
    R$\_$NET$\_$MIG$\_$2021 & Rate of change in net migration in 2021 & Percentage \\
    R$\_$DEATH$\_$2021 & The county death rate for the year 2021 & Percentage \\
    \hline
  \end{tabular}
  \caption{Variable Overview}
  \label{tab:example}
\end{table}


## Constructing the Dataset 

Combining all four socioeconomic datasets—poverty rates, population change, unemployment rates, and education levels—with the longitude-latitude coordinates for each county's seat, the final dataset (`data`) consists of the 6 aforementioned predictors. Because each csv file utilizes a different variable, all county columns are denoted `Area_Name`. 

```{r}
unemployment <- read.csv("Unemployment.csv") %>% 
  filter(State == "CA", Area_Name != "California") %>%
  spread(Attribute, Value) %>%
  select(Area_Name, Med_HH_Income_Percent_of_State_Total_2021,
         Unemployment_rate_2021) %>%
  rename("PercentStateMedianIncome" = 
           "Med_HH_Income_Percent_of_State_Total_2021") %>%
  mutate(Area_Name = str_replace(Area_Name, " County, CA", "")) %>%
  mutate(Area_Name = if_else(Area_Name == "San Francisco County/city, CA", 
                               "San Francisco", as.character(Area_Name))) 
poverty <- read.csv("PovertyEstimates.csv") %>% 
  filter(Stabr == "CA", Area_name != "California") %>%
  spread(Attribute, Value) %>%
  rename("Area_Name" = "Area_name") %>%
  select(Area_Name, PCTPOVALL_2021) %>%
  mutate(Area_Name = str_replace(Area_Name, " County", ""))
education <- read.csv("Education.csv") %>%
  filter(State == "CA", Area.name != "California") %>%
  spread(Attribute, Value)  %>%
  rename("Area_Name" = "Area.name", 
         "Bachelors_2021" = 
           "Percent of adults with a bachelor's degree or higher, 2017-21",
         "Highschool_2021" =
           "Percent of adults with a high school diploma only, 2017-21") %>%
  select(Area_Name, Bachelors_2021, Highschool_2021) %>%
  mutate(Area_Name = str_replace(Area_Name, " County", ""))
population <- read.csv("PopulationEstimates.csv") %>% 
  filter(State == "CA", Area_Name != "California") %>%
  spread(Attribute, Value) %>% 
  select(Area_Name, R_NET_MIG_2021, R_DEATH_2021) %>%
  mutate(Area_Name = str_replace(Area_Name, " County", ""))
county_loc <- 
  read.table("http://www.stat.ucla.edu/~nchristo/statistics_c173_c273/ca_seats_coord.txt", 
                header=TRUE)  %>%
  rename("Area_Name" = "county")
adj <- 
  read.table("http://www.stat.ucla.edu/~nchristo/statistics_c173_c273/county_adjacency.txt", 
                sep="\t", fill=FALSE, strip.white=TRUE)[,c(1,3)]
data <- unemployment %>% 
  left_join(poverty, by="Area_Name") %>%
  left_join(population, by="Area_Name") %>%
  left_join(education, by="Area_Name") %>%
  mutate(longitude = county_loc$longitude,
         latitude = county_loc$latitude) %>%
  select(longitude, latitude, Unemployment_rate_2021, PercentStateMedianIncome, 
         PCTPOVALL_2021, Bachelors_2021, Highschool_2021, R_NET_MIG_2021, R_DEATH_2021)
head(data)
```




\newpage




# Exploratory Data Analysis


## Mapping County Seat Locations and Bubble Plot of Unemployment

For the bubble plot, unemployment rates are classified into several levels based on the U.S. Bureau of Labor Statistics. In this project, a "Low" unemployment rate is defined as 0-6%, a "Moderate" rate is 6-10%, and a "High" rate is 10% or higher.

```{r}
par(mfrow=c(1,4))
map("county", "ca", main = "California Map of County Seats", xlim = c(-124.3,-114.2))
#title("California County Seat Locations", cex.main = 1)
points(data$longitude, data$latitude)

# Green: Low, Orange: Moderate, Red: High
colors <- c("green", "orange", "red")
# Low: 0-6, Moderate: 6-10, High: 10+
levels <- as.numeric(cut(data$Unemployment_rate_2021, 
                         c(0, 6, 10, max(data$Unemployment_rate_2021))))

map("county", "ca", main = "Map of County Seats", xlim = c(-124.3,-114.2))
title("Bubble Plot of Unemployment Rate", cex.main = 1)
points(data$longitude, data$latitude, 
       cex=(data$Unemployment_rate_2021/mean(data$Unemployment_rate_2021)), 
       col=colors[levels], pch=19)
```

Analyzing the bubble plot of unemployment rates across California, counties with major metropolitan areas or industries tend to have a "Low" unemployment rate. Specifically, these consist of counties containing or surrounding the Bay Area (San Francisco), Los Angeles, Santa Barbara, San Diego, and Sacramento. 

On the other hand, counties within Central California and nearby the Mexico border have a "High" unemployment rate. Furthermore, counties with "Medium" unemployment rates are generally in rural and low-population regions, such as Northern California and Central California. 

## Histograms of Parameters

```{r, fig.width=10, fig.height=4}
par(mfrow=c(1,3))
hist(data$Unemployment_rate_2021,main="County Unemployment Rate",xlab ="Percent")
hist(data$PercentStateMedianIncome, 
     main = "County Median Income against State Median", xlab = "Percent")
hist(data$PCTPOVALL_2021,main ="Percent of Individuals below Poverty",xlab ="Percent")
```

The county unemployment rate is right skewed, with a large majority of rates between 6-8%. Likewise, analyzing the county median income with respect to the California median, most counties are 60-80% of the state median income. Finally, the percent of individuals below the poverty line for each county is roughly normal, with the mean around 12-15%. 

```{r, fig.width=10, fig.height=4}
par(mfrow=c(1,3))
hist(data$Bachelors_2021,main="Percent with Bachelor's or Higher",xlab ="Percent")
hist(data$Highschool_2021,main ="Percent with Only High School Diploma",xlab="Percent")
hist(data$R_NET_MIG_2021,main = "Rate of County Migration",xlab = "Percent")
```

The percentage of each county's labor force with a Bachelor's degree or higher is extremely right skewed, as 15 counties contain only 15-20% of individuals with at least a Bachelor's degree. On the other hand, roughly 20 counties contain 25-30% of individuals with only a high school diploma. Finally, the rate of net migration is generally negative for most counties, suggesting these regions are losing population. 

## Scatterplots

```{r, fig.weight = 8, fig.height=8}
pairs(~ Unemployment_rate_2021 + PercentStateMedianIncome + PCTPOVALL_2021 + 
        Bachelors_2021 + latitude + longitude, data=data)
```

As the county median income with respect to the California median increases, there is a decrease in unemployment rate. On the contrary, as the percent of individuals below the poverty line increases, the unemployment rate increases. Focusing on Bachelor's degree or higher, we see a decrease in unemployment rate as the percentage of each county's labor force with a Bachelor's degree or higher increases. Generally, as the percent of Bachelor's degrees increases, there is also a decrease in percent below poverty and an increase in median income. There does not appear to be a relationship between longitude/latitude and unemployment.


## H-Scatterplot of Unemployment Rate

```{r}
data_h <- data
coordinates(data_h) <- ~longitude+latitude
qq <- hscat((Unemployment_rate_2021)~1, data=data_h, c(0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1))
plot(qq, main="h-scatterplots")
```

The correlation coefficient (r) increases when the separation distance increases from (0.1,0.2] to (0.2,0.3], and peaks at r = 0.925. However, as the separation distance increases over 0.3, we can see the coefficient decrease significantly to r = 0.583.


## Correlation Plot

```{r, out.width = "50%", out.height = "50%"}
correlation_matrix <- cor(data[, 3:9])
corrplot(correlation_matrix, method = "color")
```

There is a moderate negative correlation between the unemployment rate and the county median income with respect to the California median (`PercentStateMedianIncome`). Similarly, there is also a moderate negative correlation between unemployment rate and the percentage of each county's labor force with a Bachelor's degree or higher (`Bachelors_2021`). 

There is a moderate positive correlation between the unemployment rate and the percent of individuals below the poverty line (`PCTPOVALL_2021`). There is also a moderate positive correlation between unemployment and the percent of each county's labor force with only a high school diploma (`Highschool_2021`). 


## Box-Plot of Select Parameters

```{r, fig.width=10, fig.height=4}
par(mfrow=c(1,3))
boxplot(data$Unemployment_rate_2021,main="Unemployment Rate")
boxplot(data$PercentStateMedianIncome,main="Percent of State Median Income")
boxplot(data$Bachelors_2021,main="Percent with Bachelor's Degree")
```

We can see that the median unemployment rate is around 7%, with two outliers. The median of county income with respect to the state median is roughly 80%, with one outlier more than 160% of the state median. Finally, the median percent with a Bachelor's degree or higher is around 25%, with no outliers. 

## ECDF of Select Parameters

```{r}
par(mfrow=c(1,3))
plot(ecdf(data$Unemployment_rate_2021), main = "Unemployment Rate")
plot(ecdf(data$PCTPOVALL_2021), main = "Poverty")
plot(ecdf(data$Highschool_2021), main = "Percent with High School Only")
```

Analyzing the ECDF plots, almost all unemployment rates are between 5-10%, with 80% of unemployment rates above 9%. On the contrary, we see that the percentage of the total population living below the poverty line is mostly between 7-20%, with 60% of counties above 15%. Finally, the county-level percentage  of adults with a high school diploma only is mostly between 10-30%. 












# Lattice Data Analysis

## Adjacency Matrix

Mapping the counties and their names in California, we visually identify neighboring counties: 

```{r}
par(mfrow=c(1,4))
map("county", "ca", main = "Map of County Seats")
points(county_loc$longitude, county_loc$latitude)
text(county_loc$longitude, county_loc$latitude-0.1, labels=county_loc$Area_Name)

map("county", "ca", main = "Map of County Seats", xlim=c(-123.4,-121),ylim=c(36,39))
points(county_loc$longitude, county_loc$latitude)
text(county_loc$longitude, county_loc$latitude-0.1, labels=county_loc$Area_Name)
```

Constructing the adjacency matrix for the counties in California: 

```{r}
adj <- adj %>% mutate(V1 = replace(V1, V1 == "", NA))
adj1 <- adj %>% fill(V1,.direction = "down")
adj1 <- adj1 %>% filter(grepl("CA", V1, useBytes = TRUE)) %>%
  filter(grepl("CA", V3, useBytes = TRUE)) %>% 
  mutate(V1 = replace(V1, V1==V3, NA))
adj_df <- as.matrix(table(adj1$V1, adj1$V3))
head(adj_df,c(6,3))
```


## Moran’s I statistic

Because the mean is not constant, we compute the Moran’s $I$ test statistic using the residuals. We construct and calculate the following: $\boldsymbol{X, X'X, \hat{\beta}, H, \hat{Y}, e}$. As such, the statistic is: $$I = \frac{n}{S_0} \frac{\boldsymbol{e'we}}{\boldsymbol{e'e}}$$ We compute the Moran's $I$ statistic as follows:

```{r}
ones <- rep(1, nrow(data))
X <- as.matrix(cbind(ones, data$PercentStateMedianIncome, data$PCTPOVALL_2021, 
                     data$Bachelors_2021, data$Highschool_2021, data$R_NET_MIG_2021,
                     data$R_DEATH_2021, data$longitude))
Q <- t(X) %*% X
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% data$Unemployment_rate_2021
Yhat <- X %*% beta_hat
e <- data$Unemployment_rate_2021 - Yhat
H <- X %*% solve(t(X) %*% X) %*% t(X)
n <- 58
ones <- rep(1, nrow(adj_df))
S0 <- t(ones) %*% adj_df %*% ones
I <- (n / S0) * (t(e) %*% adj_df %*% e) / (t(e) %*% e)
I
```

Because we have a negative Moran's $I$ statistic (`-0.03992056`), this suggests that we have spatial dispersion rather than spatial clustering; thus, similar values are spread apart between counties. However, because the value is extremely close to 0, we are confident that there is spatial randomness or no spatial autocorrelation.

## Expected Moran's I Statistic

Since the mean is not constant, we define the expected Moran's $I$ statistic as: $$E[I] = -\frac{n}{S_0} \frac{tr(\boldsymbol{WH})}{n-k-1}$$ where $k$ is the number of predictors, $\boldsymbol{W}$ is the adjacency matrix, and $n$ is the number of counties.

```{r}
k <- 7
n <- 58
E_I <- -n/S0 * (sum(diag((adj_df %*% H))))/(n-k-1)
E_I
```

Because our calculated Moran's $I$ statistic is neither significantly above or below the expected Moran's $I$ statistic, this suggests that there is no significant spatial autocorrelation in the data as we mentioned above. Therefore, this implies that the distribution of values across space is essentially random, with no discernible clustering or dispersion tendencies in the data. 








\newpage








# Geostatistical Data Analysis

## Variogram Calculations using GeoR

Spatial statistics computations can be done in `R` using the package `geoR`. To use the `geoR` package we need to convert our data to a `geodata` object. We initialize our data as such:

```{r}
data_unemployment <- data %>% select(longitude, latitude, Unemployment_rate_2021)
b <- as.geodata(data_unemployment)
```


### Computing the Empirical Variogram

The variogram can be computed using the function `variog`. The robust estimator can be used with the argument `estimator.type=“modulus”`, and is robust to outliers compared to the classical estimator. We compute and plot both as such:

```{r, fig.height=6, fig.width=8, out.height="70%", out.width="70%", message=FALSE}
var1 <- variog(b, max.dist=5)
var2 <- variog(b, max.dist=5, estimator.type="modulus")

plot(var1, main = "empirical semivariogram")
points(var2$u, var2$v, col="blue")
legend("topleft", legend = c("Classical Estimator", "Robust Estimator"), 
       col = c("black", "blue"), pch=21)
```

Because the variogram function is increasing but does not appear to reach a saturation point, we are confident that the data does not have strong spatial correlation. This is consistent with our prior analysis. 

### Fitting Spherical Semivariograms to the Empirical Variogram

Having computed the variogram, we fit a function to it to compute what the variogram graph would look like if we had the entire population of all possible pairs. As such, we select the spherical model and compute the semivariogram using the following three weights: Default (npairs), Cressie, and Equal. 

```{r, fig.height=4, fig.width=10, message=FALSE}
par(mfrow=c(1,3))
plot(var1, main = "Default Spherical Semivariogram"); points(var2$u, var2$v, col="blue")
default <- variofit(var1,cov.model="sph",ini.cov.pars=c(3,1.5),fix.nugget=FALSE,nugget=0)
lines(default, lty=1, col = "magenta")
plot(var1, main = "Cressie Spherical Semivariogram"); points(var2$u, var2$v, col="blue")
cressie <- variofit(var1, cov.model="sph", weights="cressie", ini.cov.pars=c(3,1.5),
       fix.nugget=FALSE, nugget=0)
lines(cressie, lty=1, col="red")
plot(var1, main = "Equal Spherical Semivariogram"); points(var2$u, var2$v, col="blue")
equal <- variofit(var1, cov.model="sph", ini.cov.pars=c(3,1.5), weights="equal",
       fix.nugget=FALSE, nugget=0)
lines(equal, lty=1, col="darkgreen")
```

Using the spherical semivariogram, the default weights seem to fit the data the best. 

\newpage

### Fitting Exponential Semivariograms to the Empirical Variogram

Having computed the variogram, we fit a function to it to compute what the variogram graph would look like if we had the entire population of all possible pairs. As such, we select the exponential model and compute the semivariogram using the following three weights: Default (npairs), Cressie, and Equal. 

```{r, fig.height=4, fig.width=10, message=FALSE}
par(mfrow=c(1,3))
plot(var1, main = "Default Exponential Semivariogram"); points(var2$u, var2$v, col="blue")
default_exp <- variofit(var1,cov.model="exp",ini.cov.pars=c(3,1.5),fix.nugget=FALSE,nugget=0)
lines(default_exp, lty=1, col = "magenta")
plot(var1, main = "Cressie Exponential Semivariogram"); points(var2$u, var2$v, col="blue")
cressie_exp <- variofit(var1, cov.model="exp", weights="cressie", ini.cov.pars=c(3,1.5),
       fix.nugget=FALSE, nugget=0)
lines(cressie_exp, lty=1, col="red")
plot(var1, main = "Equal Exponential Semivariogram"); points(var2$u, var2$v, col="blue")
equal_exp <- variofit(var1, cov.model="exp", ini.cov.pars=c(3,1.5), weights="equal",
       fix.nugget=FALSE, nugget=0)
lines(equal_exp, lty=1, col="darkgreen")
```

For the exponential semivariogram, default weights appear to fit the data the best.

\newpage

### PRESS Calculation between Spherical and Exponential

Because we are using the `geoR` package, we can implement the `xvalid` function with `reest=TRUE` in order to compute the PRESS. As such, we then compare the PRESS for the two variograms in order to identify whether the spherical or exponential model is a better fit. **We will use the default weights (npairs).**

```{r}
## Fit the spherical variogram to the sample variogram 
fit1 <- variofit(var1, cov.model="sph",ini.cov.pars=c(3,1.5),fix.nugget=FALSE,nugget=0)
x_val1 <- xvalid(b, model=fit1,reest=TRUE,variog.obj = var1)
press1 <- sum(x_val1$error^2)
press1/58

## Fit the exponential variogram to the sample variogram 
fit2 <- variofit(var1, cov.model="exp",ini.cov.pars=c(3,1.5),fix.nugget=FALSE,nugget=0)
x_val2 <- xvalid(b, model=fit2,reest=TRUE,variog.obj = var1)
press2 <- sum(x_val2$error^2)
press2/58
```

Using the `geoR` package and the default weights, the **exponential model has a lower PRESS value**, so we will select this model to make predictions. We will also compare the PRESS for exponential and spherical using the `gstat` package. The `gstat` package utilizes a different method for spatial statistics computations. Thus, we will re-compute the empirical variogram, and fit both spherical and exponential semivariograms. The results should be similar, but will not be exactly the same. 







\newpage









## Variogram Calculations using Gstat

Spatial statistics computations can also be done in `R` using the package `Gstat`. Unlike the `geoR` package, we do not need to convert our data to a `geodata` object. We continue using the `data_unemployment` dataset. We expect that the default weight and exponential semivariogram will best fit the data for both packages.

### Computing the Empirical Variogram

We will have to take trend into account when computing the variogram. We can fit a linear surface to the data by regressing the data against the Longitude and Latitude coordinates.

```{r, fig.height=4, fig.width=10}
g_unemployment <- gstat(id="unemployment_log", formula = log(Unemployment_rate_2021)~1, 
                  locations = ~longitude+latitude, data = data_unemployment)
var_unemployment <- variogram(g_unemployment) 
g_unemployment1 <- gstat(id="unemployment_log", formula = 
                        log(Unemployment_rate_2021)~longitude+latitude, 
                      locations = ~longitude+latitude, data = data_unemployment) 
var_unemployment1 <- variogram(g_unemployment1)
plot1 <- plot(var_unemployment, main = "Empirical Variogram")
plot2 <- plot(var_unemployment1, main = "De-Trended Empirical Variogram") 
grid.arrange(plot1, plot2, nrow = 1)
```

By de-trending the data, we can see that the variogram appears to reach a saturation point. However, there does not appear to be strong spatial correlation. This is consistent with our prior analysis. 

\smallskip 

In order to fit a semivariogram to our empirical variogram above, we can apply certain weights to better fit the data. The default is $\frac{N_h}{h^2}$ where $N_h$ is the number of pairs and $h$ is the separation distance. The other semivariogram weights are as follows:

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    fit.method & Weights \\
    \hline
    1 & $N_h$ \\
    2 & $\frac{N_h}{\gamma^2 (h; \theta)}$ (Cressie's weights) \\
    6 & OLS (no weights) \\
    7 & $\frac{N_h}{h^2}$ (default) \\
    \hline
  \end{tabular}
\end{table}

\smallskip 

We will test these weights with both the spherical and exponential semi-variograms. 

### Fitting Spherical Semivariograms to the Empirical Variogram

Having computed our sample empirical variogram, we fit a function to it to compute what the variogram graph would look like if we had the entire population of all possible pairs. As such, we first select the spherical semivariogram with the four different weights: 

```{r, fig.height=4, fig.width=10, warning=FALSE}
var_fit1 <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Sph",0.5,0),fit.method=1)
plot1 <- plot(var_unemployment1, var_fit1, main = "N_h Weights") 
var_fit2 <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Sph",1,0),fit.method=2) 
plot2 <- plot(var_unemployment1, var_fit2, main = "Cressie's Weights")
grid.arrange(plot1, plot2, nrow = 1)
```

```{r, fig.height=4, fig.width=10, warning=FALSE}
var_fit6 <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Sph",1.5,0),fit.method=6) 
plot3 <- plot(var_unemployment1, var_fit6, main = "OLS (No Weights)")
var_fit7 <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Sph",1.2,0),fit.method=7) 
plot4 <- plot(var_unemployment1, var_fit7, main = "Default Weights")
grid.arrange(plot3, plot4, nrow = 1)
```

For the spherical semivariograms, the default weight ($N_h$) appears to fit the data the best, so we will use this weight for our future analysis. This is the same conclusion as when using the `geoR` package. 

\newpage 

### Fitting Exponential Semivariograms to the Empirical Variogram

Having computed the sample empirical variogram, we fit a function to it to compute what the variogram graph would look like if we had the entire population of all possible pairs. As such, we compute the exponential semivariogram with the four weights.

```{r, fig.height=4, fig.width=10, warning=FALSE}
var_fit1_exp <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Exp",0.5,0),fit.method=1)
plot1 <- plot(var_unemployment1, var_fit1_exp, main = "N_h Weights") 
var_fit2_exp <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Exp",1,0),fit.method=2) 
plot2 <- plot(var_unemployment1, var_fit2_exp, main = "Cressie's Weights")
grid.arrange(plot1, plot2, nrow = 1)
```

```{r, fig.height=4, fig.width=10, warning=FALSE}
var_fit6_exp <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Exp",1.5, 0),fit.method=6) 
plot3 <- plot(var_unemployment1, var_fit6_exp, main = "OLS (No Weights)")
var_fit7_exp <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Exp",1.2,0),fit.method=7) 
plot4 <- plot(var_unemployment1, var_fit7_exp, main = "Default Weights")
grid.arrange(plot3, plot4, nrow = 1)
```

For the exponential semivariograms, the default weight ($N_h$) still appears to fit the data the best, so we will use this weight for our future analysis. This is the same conclusion as when using the `geoR` package. 

\newpage 

### PRESS Calculation between Spherical and Exponential

Because we are using the `gstat` package, we can use the `krige.cv` function to conduct leave one out cross validation. The function automatically deletes one point at a time and uses the remaining n-1 points to predict it. We then compare the PRESS for the two variograms in order to identify whether the spherical or exponential model is a better fit. **We will use the default weights (npairs).**

```{r}
#Using spherical: 
var_fit1 <- fit.variogram(variogram(g_unemployment1), vgm(0.045,"Sph",0.5,0), 
                          fit.method=1)
cv_pr <- krige.cv(log(Unemployment_rate_2021)~1,data=data_unemployment, 
                  locations=~longitude+latitude, 
                  model=var_fit1,nfold=nrow(data_unemployment))
#names(cv_pr)
press_cv <- sum(cv_pr$residual^2)
summary(cv_pr)[,1:4]
press_cv

#Using exponential:
var_fit1_exp <- fit.variogram(variogram(g_unemployment1), vgm(0.045,"Exp",0.5,0), 
                          fit.method=1)
cv_pr1 <- krige.cv(log(Unemployment_rate_2021)~1,data=data_unemployment, 
                   locations=~longitude+latitude, 
                   model=var_fit1_exp,nfold=nrow(data_unemployment))
#names(cv_pr1)
press_cv1 <- sum(cv_pr1$residual^2)
summary(cv_pr1)[,1:4]
press_cv1
```

Using the `gstat` package and the default weights, the **exponential model has a lower PRESS value**, so we will select this model to make predictions. This is the same result as with the `geoR` package.  





\newpage







# Kriging Predictions using Exponential Semivariogram

```{r}
data_logunemployment <- data %>% 
  select(longitude, latitude, Unemployment_rate_2021) %>%
  mutate(Unemployment_rate_2021 = log(Unemployment_rate_2021)) %>%
  rename("x" = "longitude", "y" = "latitude")
data_unemployment <- data %>% select(longitude, latitude, Unemployment_rate_2021)
b <- as.geodata(data_unemployment)
x.range <- as.integer(range(data_logunemployment[,1])) 
y.range <- as.integer(range(data_logunemployment[,2])) 
grd <- expand.grid(x=seq(from=x.range[1], to=x.range[2], by=0.1), 
                   y=seq(from=y.range[1], to=y.range[2], by=0.1)) 
```

## Ordinary Kriging (gstat)

Performing ordinary kriging using `gstat` package and exponential semivariogram:

```{r, echo=FALSE}
var_fit1_exp <- fit.variogram(variogram(g_unemployment1), vgm(0.05,"Exp",0.5,0), 
                          fit.method=1)
```

```{r}
#Perform ordinary kriging predictions:
pr_ok <- krige(id="logunemployment", Unemployment_rate_2021~1, 
               locations=~x+y, model=var_fit1_exp, 
               data=data_logunemployment, newdata=grd) 

#Collapse the vector of the predicted values into a matrix: 
qqq <- matrix(pr_ok$logunemployment.pred, 
              length(seq(from=x.range[1], to=x.range[2], by=0.1)), 
              length(seq(from=y.range[1], to=y.range[2], by=0.1)) ) 
```

### Raster Map using the Predicted Values

```{r, fig.height=5, fig.width=8, out.height="70%", out.width="70%"}
par(mfrow=c(1,2))
image.orig <- image
image.orig(seq(from=x.range[1],to=x.range[2],by=0.1),
           seq(from=y.range[1],to=y.range[2], by=0.1),qqq,
           xlab="West to East",ylab="South to North", main="Predicted values")
points(data_logunemployment)
#map("county", "ca",add=TRUE)
#Identify the location of each point in the grid:
in.what.state <- map.where(database="state", x=grd$x, y=grd$y)
#Find the points of the grid that belong to California:
in.ca <- which(in.what.state=="california")
pred <- pr_ok$logunemployment.pred
#Assign NA values to all the points outside California:
pred[-in.ca] <- NA
qqq.ca <- matrix(pred, length(seq(from=x.range[1], to=x.range[2], by=0.1)),
                 length(seq(from=y.range[1], to=y.range[2], by=0.1)))
image.orig(seq(from=x.range[1],to=x.range[2],by=0.1),
           seq(from=y.range[1],to=y.range[2], by=0.1),qqq.ca,
           xlab="West to East",ylab="South to North", main="Predicted values")
contour(seq(from=x.range[1], to=x.range[2], by=0.1),
        seq(from=y.range[1],to=y.range[2], by=0.1), qqq.ca, add=TRUE,
        col="black", labcex=1)
```

```{r, fig.height=5, fig.width=5, out.height="55%", out.width="55%", message=FALSE}
filled.contour(seq(from=x.range[1],to=x.range[2],by=0.1),
               seq(from=y.range[1],to=y.range[2], by=0.1),qqq.ca,
               xlab="West to East",ylab="South to North", main="Predicted values",
               col=heat.colors(10))
```


We can see that the Ordinary Kriging raster map is fairly similar to our bubble plot from the Explanatory Data Analysis. Counties within Central California and nearby the Mexico border have a more extreme unemployment rate. We will also analyze the Universal Kriging and Co-Kriging models with the exponential semivariogram. We will use the Ordinary Kriging for our Co-Kriging model.

\newpage


## Universal Kriging (geoR)

Performing universal kriging using `geoR` package and exponential semivariogram:

```{r,include=TRUE, eval=TRUE}
fit2 <- variofit(var2,cov.model="exp",ini.cov.pars=c(3,1.5),fix.nugget=FALSE,nugget=0)
kc <- krige.conv(b, locations=grd, krige=krige.control(obj.model=fit2),
                 nugget=0, trend.l="1st", trend.d="1st")
```


```{r,include=FALSE, eval=FALSE}
#univ_kr <- krige(id="logunemployment", Unemployment_rate_2021~x+y, 
               #locations=~x+y, model=var_fit1, 
               #data=data_logunemployment, newdata=grd) 
#qqq <- matrix(univ_kr$logunemployment.pred, 
              #length(seq(from=x.range[1], to=x.range[2], by=0.1)), 
              #length(seq(from=y.range[1], to=y.range[2], by=0.1)) ) 
```

### Raster Map using the Predicted Values

```{r, fig.height=5, fig.width=8, out.height="70%", out.width="70%"}
par(mfrow=c(1,2))
image.orig <- image
image.orig(seq(from=x.range[1],to=x.range[2],by=0.1),seq(from=y.range[1],to=y.range[2],by=0.1),
           qqq,xlab="West to East",ylab="South to North", main="Predicted values")
points(data_unemployment)
in.what.state <- map.where(database="state", x=grd$x, y=grd$y)
in.ca <- which(in.what.state=="california")
pred <- kc$predict; pred[-in.ca] <- NA
qqq.ca <- matrix(pred, length(seq(from=x.range[1], to=x.range[2], by=0.1)),
                 length(seq(from=y.range[1], to=y.range[2], by=0.1)))
image.orig(seq(from=x.range[1],to=x.range[2],by=0.1),
           seq(from=y.range[1],to=y.range[2], by=0.1),qqq.ca,
           xlab="West to East",ylab="South to North", main="Predicted values")
contour(seq(from=x.range[1], to=x.range[2], by=0.1),
        seq(from=y.range[1],to=y.range[2], by=0.1),qqq.ca,add=TRUE,col="black",labcex=1)
```

```{r, fig.height=5, fig.width=5, out.height="55%", out.width="55%", message=FALSE}
filled.contour(seq(from=x.range[1],to=x.range[2],by=0.1),
               seq(from=y.range[1],to=y.range[2], by=0.1),qqq.ca,
               xlab="West to East",ylab="South to North", main="Predicted values",
               col=heat.colors(10))
```

Analyzing the raster map for Universal Kriging with the exponential semivariogram, we can see that there are two primary areas with an extreme unemployment rate: Central California and the border with Mexico. However, unlike with our Ordinary Kriging raster map, this does not fully encapsulate the region, and we do not believe it is as accurate as the Ordinary model. Thus, we will implement our Co-Kriging model using the Ordinary Kriging. This difference may also be due to the fact that we are using the `geoR` package.



## Co-Kriging (gstat)

Performing co-kriging using `gstat` package and **Ordinary Kriging**:

```{r, echo=FALSE}
var_fit1_exp <- fit.variogram(variogram(g_unemployment1), vgm(0.05,"Exp",0.5,0), 
                          fit.method=1)
```

```{r}
data_cokrig <- data %>%
  select(longitude, latitude, Unemployment_rate_2021,
         PercentStateMedianIncome, PCTPOVALL_2021, Bachelors_2021,Highschool_2021) %>% 
  mutate(Unemployment_rate_2021 = log(Unemployment_rate_2021),
         PercentStateMedianIncome = log(PercentStateMedianIncome),
         PCTPOVALL_2021 = log(PCTPOVALL_2021),
         Bachelors_2021 = log(Bachelors_2021),
         Highschool_2021 = log(Highschool_2021)) %>% 
  rename("x" = "longitude", "y" = "latitude")
x.range <- as.integer(range(data_cokrig[,1])) 
y.range <- as.integer(range(data_cokrig[,2])) 
grd <- expand.grid(x=seq(from=x.range[1], to=x.range[2], by=0.1), 
                   y=seq(from=y.range[1], to=y.range[2], by=0.1)) 
#target variable
g1 <- gstat(id="log_unemployment", formula = (Unemployment_rate_2021)~1, 
            locations = ~x+y, data = data_cokrig) 
g1 <- gstat(g1,id="log_percentmedian", formula = (PercentStateMedianIncome)~1, 
            locations = ~x+y, data = data_cokrig)
g1 <- gstat(g1,id="log_poverty", formula = (PCTPOVALL_2021)~1, 
            locations = ~x+y, data = data_cokrig)
g1 <- gstat(g1,id="log_bachelors", formula = (Bachelors_2021)~1, 
            locations = ~x+y, data = data_cokrig)
g1 <- gstat(g1,id="log_highschool", formula = (Highschool_2021)~1, 
            locations = ~x+y, data = data_cokrig)
vm <- variogram(g1) 
vm.fit_exp <- fit.lmc(vm, g1, model=var_fit1_exp,correct.diagonal=1.01)
ck <- predict(vm.fit_exp, grd)
qqq <- matrix(ck$log_unemployment.pred,
              length(seq(from=x.range[1], to=x.range[2], by=0.1)),
              length(seq(from=y.range[1], to=y.range[2], by=0.1)))
```

For our Co-Kriging model, we have decided to implement the following predictors that we believe are associated with our target variable `Unemployment_rate_2021`. We have log-transformed all variables. 

- `log_percentmedian`: the log median income as a percentage of total median income for CA
- `log_poverty`: the log percentage of the total population living below the poverty line
- `log_bachelors`: the log percentage of adults with a bachelor’s degree or higher
- `log_highschool`: the log county-level percentage of adults with a high school diploma only

Having selected these four predictors, we seek to construct a predictive model that best represents the unemployment rate throughout California counties. We believe that this model will be the most accurate, as we incorporate more factors to boost our prediction.


### Raster Map using the Predicted Values

```{r, fig.height=5, fig.width=8, out.height="70%", out.width="70%"}
par(mfrow=c(1,2))
image.orig <- image
image.orig(seq(from=x.range[1],to=x.range[2],by=0.1),
           seq(from=y.range[1],to=y.range[2], by=0.1),qqq,
           xlab="West to East",ylab="South to North", main="Predicted values")
points(data_cokrig)
in.what.state <- map.where(database="state", x=grd$x, y=grd$y)
in.ca <- which(in.what.state=="california")
pred <- ck$log_unemployment.pred
pred[-in.ca] <- NA
qqq.ca <- matrix(pred, length(seq(from=x.range[1], to=x.range[2], by=0.1)),
                 length(seq(from=y.range[1], to=y.range[2], by=0.1)))
image.orig(seq(from=x.range[1],to=x.range[2],by=0.1),
           seq(from=y.range[1],to=y.range[2], by=0.1),qqq.ca,
           xlab="West to East",ylab="South to North", main="Predicted values")
contour(seq(from=x.range[1], to=x.range[2], by=0.1),
        seq(from=y.range[1],to=y.range[2], by=0.1), qqq.ca, add=TRUE,
        col="black", labcex=1)
```

```{r, fig.height=5, fig.width=5, out.height="55%", out.width="55%", message=FALSE}
filled.contour(seq(from=x.range[1],to=x.range[2],by=0.1),
               seq(from=y.range[1],to=y.range[2], by=0.1),qqq.ca,
               xlab="West to East",ylab="South to North", main="Predicted values",
               col=heat.colors(10))
```


Comparing the three Kriging models—ordinary, universal, and co-kriging—we can see that the co-kriging model best emulates the bubble plot from our explanatory data analysis when using the exponential semivariogram. We can see that areas in Central California and around the Mexico border tend to have a higher unemployment rate. Furthermore, parts of Northern California also have higher rates compared to the rest of the state.

\smallskip

Therefore, we believe that co-kriging is the best predictive model, with ordinary and universal kriging as the second and third best models. We will utilize cross-validation in order to enumerate their accuracy.





\newpage




# Kriging Cross-Validation

For our final section, we compute the sample variogram and fit the spherical and exponential variograms to it. While we exclusively utilize the exponential variogram in the previous section, we will compute all kriging PRESS values for both to validate our assumption. Then, using leave one out cross-validation (`krige.cv`), we predict the points and compare the prediction sum of squares (PRESS) for each variogram. 

## PRESS Calculations for Kriging using Spherical Variogram

We first focus on computing the cross-validation metrics for the following Kriging models: Ordinary, Universal, and Co-Kriging. We will use the `gstat` package and leave-one-out cross-validation (`krige.cv`).  

```{r, echo=FALSE}
var_fit1 <- fit.variogram(variogram(g_unemployment1), vgm(0.045,"Sph",0.5,0), 
                          fit.method=1)
```


### Ordinary Kriging PRESS for Spherical
```{r}
pr_ok <- krige.cv(Unemployment_rate_2021~1, data=data_logunemployment,
               locations=~x+y, model=var_fit1, nfold = nrow(data_logunemployment)) 
PRESS_ok <- sum(pr_ok$residual^2) / nrow(data_logunemployment)
```

The PRESS for ordinary kriging is:

```{r, echo=FALSE}
PRESS_ok
```

### Universal Kriging PRESS for Spherical
```{r}
pr_uk <- krige.cv(Unemployment_rate_2021~x+y, data=data_logunemployment,
               locations=~x+y, model=var_fit1, nfold = nrow(data_logunemployment)) 
PRESS_uk <- sum(pr_uk$residual^2) / nrow(data_logunemployment)
```

The PRESS for universal kriging is:

```{r, echo=FALSE}
PRESS_uk
```

### Co-Kriging PRESS for Spherical
```{r, message=FALSE,include=FALSE}
var_fit1 <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Sph",0.4,0),fit.method=1)
vm <- variogram(g1) 
vm.fit_sph <- fit.lmc(vm, g1, model=var_fit1,correct.diagonal=1.01)
cv_ck_sph <- gstat.cv(vm.fit_sph)
PRESS_ck_sph <- sum(cv_ck_sph$residual^2)/nrow(data_cokrig)
```

```{r, eval=FALSE}
vm.fit_sph <- fit.lmc(vm, g1, model=var_fit1,correct.diagonal=1.01)
cv_ck_sph <- gstat.cv(vm.fit_sph)
PRESS_ck_sph <- sum(cv_ck_sph$residual^2)/nrow(data_cokrig)
```

The PRESS for co-kriging is:

```{r, echo=FALSE}
PRESS_ck_sph
```

Because it has the lowest PRESS value, we would recommend choosing the co-kriging model specifically when using the spherical semivariogram. However, because we believe that the exponential variogram will be more accurate, so we will need to calculate the PRESS for all three models again when using this model. We expect the PRESS calculations for kriging using the exponential variogram to be lower.






## PRESS Calculations for Kriging using Exponential Variogram

Having caluclated the PRESS for Kriging using the spherical variogram, we compute the cross-validation metrics for the following Kriging models using the exponential variogram: Ordinary, Universal, and Co-Kriging. We expect these values to be lower.   

```{r, echo=FALSE}
var_fit1_exp <- fit.variogram(variogram(g_unemployment1), vgm(0.05,"Exp",0.5,0), 
                          fit.method=1)
```

### Ordinary Kriging PRESS for Exponential
```{r}
#Compute PRESS for universal kriging with the exponential semivariogram
pr_ok_exp <- krige.cv(Unemployment_rate_2021~1, data=data_logunemployment,
               locations=~x+y, model=var_fit1_exp, nfold = nrow(data_logunemployment)) 
PRESS_ok_exp <- sum(pr_ok_exp$residual^2) / nrow(data_logunemployment)
```

The PRESS for ordinary kriging is:

```{r, echo=FALSE}
PRESS_ok_exp
```

### Universal Kriging PRESS for Exponential
```{r}
pr_uk_exp <- krige.cv(Unemployment_rate_2021~x+y, data=data_logunemployment,
               locations=~x+y, model=var_fit1_exp, nfold = nrow(data_logunemployment)) 
PRESS_uk_exp <- sum(pr_uk_exp$residual^2) / nrow(data_logunemployment)
```

The PRESS for universal kriging is:

```{r, echo=FALSE}
PRESS_uk_exp
```

### Co-Kriging PRESS for Exponential
```{r, message=FALSE,include=FALSE}
var_fit1_exp <- fit.variogram(variogram(g_unemployment1),vgm(0.045,"Exp",0.5,0),fit.method=1)
vm <- variogram(g1) 
vm.fit_exp <- fit.lmc(vm, g1, model=var_fit1_exp,correct.diagonal=1.01)
cv_ck_exp <- gstat.cv(vm.fit_exp)
PRESS_ck_exp <- sum(cv_ck_exp$residual^2)/nrow(data_cokrig)
```

```{r, eval=FALSE}
vm.fit_exp <- fit.lmc(vm, g1, model=var_fit1_exp,correct.diagonal=1.01)
cv_ck_exp <- gstat.cv(vm.fit_exp)
PRESS_ck_exp <- sum(cv_ck_exp$residual^2)/nrow(data_cokrig)
```

The PRESS for co-kriging is:

```{r, echo=FALSE}
PRESS_ck_exp
```

Because it has the lowest PRESS value, we would recommend choosing the co-kriging model when using the exponential semivariogram. For ordinary and universal kriging, the exponential model is better than the spherical model. Furthermore, the exponential co-kriging is better than the spherical one. 



